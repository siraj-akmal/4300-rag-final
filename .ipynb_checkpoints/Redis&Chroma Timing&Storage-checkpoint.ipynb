{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5049e2b5-4f24-449a-8583-f9c851238a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an embedding model:\n",
      "1: nomic-embed-text\n",
      "2: jina-embeddings-v2-base-en\n",
      "3: granite-embedding:278m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to your choice:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an LLM model:\n",
      "1: llama3.2:latest\n",
      "2: mistral\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to your choice:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM model: mistral\n",
      "Index created successfully.\n",
      "Stored embedding for: AWS Intro_chunk0.txt\n",
      "Stored embedding for: AWS Intro_chunk1.txt\n",
      "Stored embedding for: AWS Intro_chunk2.txt\n",
      "Stored embedding for: AWS Intro_chunk3.txt\n",
      "Stored embedding for: AWS Intro_chunk4.txt\n",
      "Stored embedding for: AWS Intro_chunk5.txt\n",
      "Stored embedding for: B+Tree Walkthrough_chunk0.txt\n",
      "Stored embedding for: B+Tree Walkthrough_chunk1.txt\n",
      "Stored embedding for: B+Tree Walkthrough_chunk2.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk0.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk1.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk10.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk11.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk12.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk13.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk14.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk15.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk16.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk17.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk18.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk19.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk2.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk20.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk21.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk22.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk23.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk3.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk4.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk5.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk6.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk7.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk8.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk9.txt\n",
      "Stored embedding for: B-trees_chunk0.txt\n",
      "Stored embedding for: B-trees_chunk1.txt\n",
      "Stored embedding for: B-trees_chunk2.txt\n",
      "Stored embedding for: B-trees_chunk3.txt\n",
      "Stored embedding for: B-trees_chunk4.txt\n",
      "Stored embedding for: B-trees_chunk5.txt\n",
      "Stored embedding for: BST_chunk0.txt\n",
      "Stored embedding for: BST_chunk1.txt\n",
      "Stored embedding for: BST_chunk10.txt\n",
      "Stored embedding for: BST_chunk11.txt\n",
      "Stored embedding for: BST_chunk12.txt\n",
      "Stored embedding for: BST_chunk2.txt\n",
      "Stored embedding for: BST_chunk3.txt\n",
      "Stored embedding for: BST_chunk4.txt\n",
      "Stored embedding for: BST_chunk5.txt\n",
      "Stored embedding for: BST_chunk6.txt\n",
      "Stored embedding for: BST_chunk7.txt\n",
      "Stored embedding for: BST_chunk8.txt\n",
      "Stored embedding for: BST_chunk9.txt\n",
      "Stored embedding for: BST_hw_chunk0.txt\n",
      "Stored embedding for: BST_hw_chunk1.txt\n",
      "Stored embedding for: BST_hw_chunk2.txt\n",
      "Stored embedding for: BST_hw_chunk3.txt\n",
      "Stored embedding for: Data Replication_chunk0.txt\n",
      "Stored embedding for: Data Replication_chunk1.txt\n",
      "Stored embedding for: Data Replication_chunk2.txt\n",
      "Stored embedding for: Data Replication_chunk3.txt\n",
      "Stored embedding for: Data Replication_chunk4.txt\n",
      "Stored embedding for: Data Replication_chunk5.txt\n",
      "Stored embedding for: Data Replication_chunk6.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk0.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk1.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk2.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk3.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk4.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk5.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk6.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk7.txt\n",
      "Stored embedding for: EC2 & Lambda_chunk0.txt\n",
      "Stored embedding for: EC2 & Lambda_chunk1.txt\n",
      "Stored embedding for: EC2 & Lambda_chunk2.txt\n",
      "Stored embedding for: EC2 & Lambda_chunk3.txt\n",
      "Stored embedding for: Foundations_chunk0.txt\n",
      "Stored embedding for: Foundations_chunk1.txt\n",
      "Stored embedding for: Foundations_chunk2.txt\n",
      "Stored embedding for: Foundations_chunk3.txt\n",
      "Stored embedding for: Foundations_chunk4.txt\n",
      "Stored embedding for: Foundations_chunk5.txt\n",
      "Stored embedding for: Introduction to Graph Data Model_chunk0.txt\n",
      "Stored embedding for: Introduction to Graph Data Model_chunk1.txt\n",
      "Stored embedding for: Introduction to Graph Data Model_chunk2.txt\n",
      "Stored embedding for: Introduction to Graph Data Model_chunk3.txt\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:62178/embedding\": read tcp 127.0.0.1:62182->127.0.0.1:62178: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping MongoDB Aggregation_chunk0.txt due to embedding error.\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:62195/embedding\": read tcp 127.0.0.1:62197->127.0.0.1:62195: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping MongoDB Aggregation_chunk1.txt due to embedding error.\n",
      "Stored embedding for: MongoDB Aggregation_chunk2.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk0.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk1.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk10.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk11.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk2.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk3.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk4.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk5.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk6.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk7.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk8.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk9.txt\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:62202/embedding\": read tcp 127.0.0.1:62204->127.0.0.1:62202: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping MongoDB Examples_chunk0.txt due to embedding error.\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:62209/embedding\": read tcp 127.0.0.1:62211->127.0.0.1:62209: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping MongoDB Examples_chunk1.txt due to embedding error.\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:62215/embedding\": read tcp 127.0.0.1:62217->127.0.0.1:62215: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping MongoDB Examples_chunk2.txt due to embedding error.\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:62226/embedding\": read tcp 127.0.0.1:62228->127.0.0.1:62226: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping MongoDB Examples_chunk3.txt due to embedding error.\n",
      "Stored embedding for: MongoDB Examples_chunk4.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk0.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk1.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk2.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk3.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk4.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk5.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk6.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk7.txt\n",
      "Stored embedding for: Neo4j_chunk0.txt\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:62232/embedding\": read tcp 127.0.0.1:62234->127.0.0.1:62232: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping Neo4j_chunk1.txt due to embedding error.\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:62239/embedding\": read tcp 127.0.0.1:62241->127.0.0.1:62239: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping Neo4j_chunk2.txt due to embedding error.\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:62245/embedding\": read tcp 127.0.0.1:62247->127.0.0.1:62245: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping Neo4j_chunk3.txt due to embedding error.\n",
      "Stored embedding for: Neo4j_chunk4.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk0.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk1.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk2.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk3.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk4.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk5.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk6.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk7.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk8.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk0.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk1.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk10.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk11.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk2.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk3.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk4.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk5.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk6.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk7.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk8.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk9.txt\n",
      "Stored embedding for: PyMongo_chunk0.txt\n",
      "Stored embedding for: PyMongo_chunk1.txt\n",
      "Stored embedding for: Redis + Python_chunk0.txt\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:62252/embedding\": read tcp 127.0.0.1:62254->127.0.0.1:62252: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping Redis + Python_chunk1.txt due to embedding error.\n",
      "Stored embedding for: Redis + Python_chunk2.txt\n",
      "\n",
      "Total embedding time: 104.48 seconds\n",
      "Memory used for embeddings: 5.53 MB\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What question do you want to ask?  what is an AVL tree?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 2 matching chunks retrieved:\n",
      "\n",
      "Chunk 1: tree works. This would be the “ tree”, or a tree of order 3. Figure 12.6.2: An example of building a tree Next, let’s see how to search. Figure 12.6.3: An example of searching a tree Finally, let’s see an example of deleting from the tree B+ B+ 2 −3+ B+ 1 / 28 << < > >> Example 2-3+ Tree Visualizati...\n",
      "\n",
      "Chunk 2: on either side of the subtree pointer. (This generalizes the BST invariant.) 5. The root has at least two children if it is not a leaf. For example, the following is an order-5 B-tree (m=5) where the leaves have enough space to store up to 3 data records: Because the height of the tree is uniformly ...\n",
      "\n",
      "LLM query time: 48.67 seconds\n",
      "\n",
      "Response from mistral:\n",
      " An AVL tree (Adelson-Velsky and Landis tree) is a self-balancing binary search tree, which was invented to solve the problem of maintaining the height balance in a Binary Search Tree (BST). It's named after Georgy Adelson-Marsálík (George Adelson-Velsky in English) and Evgeniy Landis.\n",
      "\n",
      "The main characteristic of AVL trees is that they maintain the height difference between the two subtrees of each node within a specific range, usually -1 and 1. This ensures that the tree remains approximately balanced, which leads to efficient operations such as insertion, deletion, and search with an average time complexity of O(log n).\n",
      "\n",
      "In an AVL tree, every node has a balance factor associated with it, indicating the height difference between its left and right subtrees. The balancing operations involve performing rotations (left rotation, right rotation, double rotation) to maintain the balance, as needed during insertion or deletion.\n",
      "\n",
      "This property of maintaining the height balance makes AVL trees slower than red-black trees or splay trees for common search operations like insertion and deletion due to their additional balancing steps. However, AVL trees have a constant factor advantage in the best case scenario (tree is already balanced), making them faster than red-black trees and splay trees in this particular situation.\n",
      "\n",
      "AVL trees are less commonly used in modern computer science compared to other self-balancing binary search trees like red-black trees or splay trees, but they are still an important data structure conceptually due to their historical significance and the algorithms they introduce.\n",
      "\n",
      "LLM query execution time: 48.75 seconds\n",
      "Memory used for LLM query: 0.05 MB\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import redis\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "from redis.commands.search.query import Query\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Initialize Redis connection\n",
    "redis_client = redis.Redis(host=\"localhost\", port=6379, db=0)\n",
    "\n",
    "# set constants\n",
    "VECTOR_DIM = 768\n",
    "INDEX_NAME = \"embedding_index\"\n",
    "DOC_PREFIX = \"doc:\"\n",
    "DISTANCE_METRIC = \"COSINE\"\n",
    "TEXT_FOLDER = \"processed_texts\"  \n",
    "selected_model = None\n",
    "jina_model = None\n",
    "selected_llm_model = None\n",
    "\n",
    "# Function to get the current memory usage of Redis in MB\n",
    "def get_redis_memory_usage():\n",
    "    memory_info = redis_client.info(\"memory\")\n",
    "    return memory_info.get(\"used_memory\", 0) / (1024 * 1024)  # Convert bytes to MB\n",
    "\n",
    "# Function to get the current memory usage of the system in MB\n",
    "def get_system_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)  # Convert bytes to MB\n",
    "\n",
    "# Clear redis database if reindexing\n",
    "def create_hnsw_index():\n",
    "    try:\n",
    "        redis_client.execute_command(f\"FT.DROPINDEX {INDEX_NAME} DD\")\n",
    "    except redis.exceptions.ResponseError:\n",
    "        pass\n",
    "    \n",
    "    redis_client.execute_command(\n",
    "        f\"\"\"\n",
    "        FT.CREATE {INDEX_NAME} ON HASH PREFIX 1 {DOC_PREFIX}\n",
    "        SCHEMA text TEXT\n",
    "        embedding VECTOR HNSW 6 DIM {VECTOR_DIM} TYPE FLOAT32 DISTANCE_METRIC {DISTANCE_METRIC}\n",
    "        \"\"\"\n",
    "    )\n",
    "    print(\"Index created successfully.\")\n",
    "\n",
    "# Generate embedding for the text\n",
    "def get_embedding(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Generate an embedding for the given text using the selected embedding model.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of floats representing the embedding vector for the input text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "            return jina_model.encode([text])[0].tolist()\n",
    "        else:\n",
    "            response = ollama.embeddings(model=EMBEDDING_MODEL, prompt=text)\n",
    "            return response[\"embedding\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for text: {e}\")\n",
    "        return None  # Return None if there's an error\n",
    "\n",
    "# Store embedding in Redis\n",
    "def store_embedding(doc_id: str, text: str, embedding: list):\n",
    "    if embedding is not None:  # Only store if embedding is valid\n",
    "        key = f\"{DOC_PREFIX}{doc_id}\"\n",
    "        redis_client.hset(\n",
    "            key,\n",
    "            mapping={\n",
    "                \"text\": text,\n",
    "                \"embedding\": np.array(embedding, dtype=np.float32).tobytes(),  # Store as byte array\n",
    "            },\n",
    "        )\n",
    "        print(f\"Stored embedding for: {doc_id}\")\n",
    "    else:\n",
    "        print(f\"Skipping file {doc_id} due to embedding generation error.\")\n",
    "\n",
    "# Process all text files and generate/store embeddings\n",
    "def process_text_files():\n",
    "    \"\"\"\n",
    "    This function processes all text files in the specified folder, reads their content,\n",
    "    generates embeddings for the text using the selected embedding model, and stores the\n",
    "    embeddings along with the text content in Redis.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(TEXT_FOLDER):\n",
    "        print(f\"Folder '{TEXT_FOLDER}' not found.\")\n",
    "        return\n",
    "\n",
    "    text_files = [f for f in os.listdir(TEXT_FOLDER) if f.endswith(\".txt\")]\n",
    "    if not text_files:\n",
    "        print(\"No text files found.\")\n",
    "        return\n",
    "\n",
    "    # Record start time for embedding process\n",
    "    start_embedding_time = time.time()\n",
    "    \n",
    "    # Get initial Redis memory usage\n",
    "    initial_memory = get_redis_memory_usage()\n",
    "\n",
    "    for filename in text_files:\n",
    "        filepath = os.path.join(TEXT_FOLDER, filename)\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "                embedding = get_embedding(text)\n",
    "                if embedding is not None:\n",
    "                    store_embedding(filename, text, embedding)\n",
    "                else:\n",
    "                    print(f\"Skipping {filename} due to embedding error.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            continue  # Skip this file and continue with the next one\n",
    "\n",
    "    # Record end time for embedding process\n",
    "    end_embedding_time = time.time()\n",
    "\n",
    "    # Calculate total embedding time\n",
    "    embedding_time = end_embedding_time - start_embedding_time\n",
    "    print(f\"\\nTotal embedding time: {embedding_time:.2f} seconds\")\n",
    "    \n",
    "    # Get final Redis memory usage\n",
    "    final_memory = get_redis_memory_usage()\n",
    "\n",
    "    # Calculate memory usage in MB\n",
    "    memory_used = final_memory - initial_memory\n",
    "    print(f\"Memory used for embeddings: {memory_used:.2f} MB\")\n",
    "\n",
    "# Perform KNN search and track memory usage during LLM query\n",
    "def perform_knn_search(query_text: str, k: int = 2):\n",
    "    \"\"\"\n",
    "    Perform a K-Nearest Neighbors (KNN) search on the Redis index using the given query text.\n",
    "\n",
    "    Parameters:\n",
    "    query_text (str): The text query to search for in the index.\n",
    "    k (int, optional): The number of nearest neighbors to retrieve. Defaults to 2.\n",
    "    \"\"\"\n",
    "    # Get initial system memory before performing the LLM query\n",
    "    initial_system_memory = get_system_memory_usage()\n",
    "\n",
    "    # Start timer for LLM query execution\n",
    "    start_query_time = time.time()\n",
    "\n",
    "    # Perform the KNN search\n",
    "    embedding = get_embedding(query_text)\n",
    "    q = (\n",
    "        Query(f\"*=>[KNN {k} @embedding $vec AS vector_distance]\")\n",
    "        .sort_by(\"vector_distance\")\n",
    "        .return_fields(\"text\", \"vector_distance\")\n",
    "        .dialect(2)\n",
    "    )\n",
    "    res = redis_client.ft(INDEX_NAME).search(\n",
    "        q, query_params={\"vec\": np.array(embedding, dtype=np.float32).tobytes()}\n",
    "    )\n",
    "    matching_chunks = [doc.text for doc in res.docs]\n",
    "    \n",
    "    if not matching_chunks:\n",
    "        print(\"No relevant matches found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nTop {len(matching_chunks)} matching chunks retrieved:\")\n",
    "    for i, chunk in enumerate(matching_chunks):\n",
    "        print(f\"\\nChunk {i+1}: {chunk[:300]}...\")  # Display first 300 characters\n",
    "\n",
    "    # Get response from the LLM\n",
    "    response = query_llm(query_text, matching_chunks)\n",
    "    print(f\"\\nResponse from {LLM_MODEL}:\\n{response}\\n\")\n",
    "\n",
    "    # Stop timer for LLM query execution\n",
    "    end_query_time = time.time()\n",
    "\n",
    "    # Calculate time taken for LLM query\n",
    "    query_time = end_query_time - start_query_time\n",
    "    print(f\"LLM query execution time: {query_time:.2f} seconds\")  # This should be printed only once\n",
    "\n",
    "    # Get final system memory after performing the LLM query\n",
    "    final_system_memory = get_system_memory_usage()\n",
    "\n",
    "    # Calculate memory usage for the LLM query in MB\n",
    "    memory_used_for_query = final_system_memory - initial_system_memory\n",
    "    print(f\"Memory used for LLM query: {memory_used_for_query:.2f} MB\")\n",
    "\n",
    "\n",
    "# Prompt user to select an embedding model\n",
    "embedding_models = {\n",
    "    \"1\": \"nomic-embed-text\",\n",
    "    \"2\": \"jina-embeddings-v2-base-en\",\n",
    "    \"3\": \"granite-embedding:278m\",\n",
    "}\n",
    "\n",
    "print(\"Select an embedding model:\")\n",
    "for key, model in embedding_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_model not in embedding_models:\n",
    "    selected_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "EMBEDDING_MODEL = embedding_models[selected_model]\n",
    "\n",
    "# If Jina embeddings are selected, load the model\n",
    "if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "    jina_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True)\n",
    "\n",
    "# Prompt user to select an LLM model\n",
    "llm_models = {\n",
    "    \"1\": \"llama3.2:latest\",\n",
    "    \"2\": \"mistral\",\n",
    "}\n",
    "\n",
    "print(\"Select an LLM model:\")\n",
    "for key, model in llm_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_llm_model not in llm_models:\n",
    "    selected_llm_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "LLM_MODEL = llm_models[selected_llm_model]\n",
    "print(f\"Using LLM model: {LLM_MODEL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_hnsw_index()\n",
    "    process_text_files()\n",
    "    query = input(\"What question do you want to ask? \")\n",
    "    perform_knn_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ba5652f-7207-45ff-89ef-99a6cde4efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an embedding model:\n",
      "1: nomic-embed-text\n",
      "2: jina-embeddings-v2-base-en\n",
      "3: granite-embedding:278m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an LLM model:\n",
      "1: llama3.2:latest\n",
      "2: mistral\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to your choice:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM model: mistral\n",
      "Stored embedding for: AWS Intro_chunk0.txt\n",
      "Stored embedding for: AWS Intro_chunk1.txt\n",
      "Stored embedding for: AWS Intro_chunk2.txt\n",
      "Stored embedding for: AWS Intro_chunk3.txt\n",
      "Stored embedding for: AWS Intro_chunk4.txt\n",
      "Stored embedding for: AWS Intro_chunk5.txt\n",
      "Stored embedding for: B+Tree Walkthrough_chunk0.txt\n",
      "Stored embedding for: B+Tree Walkthrough_chunk1.txt\n",
      "Stored embedding for: B+Tree Walkthrough_chunk2.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk0.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk1.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk10.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk11.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk12.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk13.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk14.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk15.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk16.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk17.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk18.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk19.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk2.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk20.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk21.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk22.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk23.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk3.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk4.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk5.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk6.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk7.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk8.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk9.txt\n",
      "Stored embedding for: B-trees_chunk0.txt\n",
      "Stored embedding for: B-trees_chunk1.txt\n",
      "Stored embedding for: B-trees_chunk2.txt\n",
      "Stored embedding for: B-trees_chunk3.txt\n",
      "Stored embedding for: B-trees_chunk4.txt\n",
      "Stored embedding for: B-trees_chunk5.txt\n",
      "Stored embedding for: BST_chunk0.txt\n",
      "Stored embedding for: BST_chunk1.txt\n",
      "Stored embedding for: BST_chunk10.txt\n",
      "Stored embedding for: BST_chunk11.txt\n",
      "Stored embedding for: BST_chunk12.txt\n",
      "Stored embedding for: BST_chunk2.txt\n",
      "Stored embedding for: BST_chunk3.txt\n",
      "Stored embedding for: BST_chunk4.txt\n",
      "Stored embedding for: BST_chunk5.txt\n",
      "Stored embedding for: BST_chunk6.txt\n",
      "Stored embedding for: BST_chunk7.txt\n",
      "Stored embedding for: BST_chunk8.txt\n",
      "Stored embedding for: BST_chunk9.txt\n",
      "Stored embedding for: BST_hw_chunk0.txt\n",
      "Stored embedding for: BST_hw_chunk1.txt\n",
      "Stored embedding for: BST_hw_chunk2.txt\n",
      "Stored embedding for: BST_hw_chunk3.txt\n",
      "Stored embedding for: Data Replication_chunk0.txt\n",
      "Stored embedding for: Data Replication_chunk1.txt\n",
      "Stored embedding for: Data Replication_chunk2.txt\n",
      "Stored embedding for: Data Replication_chunk3.txt\n",
      "Stored embedding for: Data Replication_chunk4.txt\n",
      "Stored embedding for: Data Replication_chunk5.txt\n",
      "Stored embedding for: Data Replication_chunk6.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk0.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk1.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk2.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk3.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk4.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk5.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk6.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk7.txt\n",
      "Stored embedding for: EC2 & Lambda_chunk0.txt\n",
      "Stored embedding for: EC2 & Lambda_chunk1.txt\n",
      "Stored embedding for: EC2 & Lambda_chunk2.txt\n",
      "Stored embedding for: EC2 & Lambda_chunk3.txt\n",
      "Stored embedding for: Foundations_chunk0.txt\n",
      "Stored embedding for: Foundations_chunk1.txt\n",
      "Stored embedding for: Foundations_chunk2.txt\n",
      "Stored embedding for: Foundations_chunk3.txt\n",
      "Stored embedding for: Foundations_chunk4.txt\n",
      "Stored embedding for: Foundations_chunk5.txt\n",
      "Stored embedding for: Introduction to Graph Data Model_chunk0.txt\n",
      "Stored embedding for: Introduction to Graph Data Model_chunk1.txt\n",
      "Stored embedding for: Introduction to Graph Data Model_chunk2.txt\n",
      "Stored embedding for: Introduction to Graph Data Model_chunk3.txt\n",
      "Stored embedding for: MongoDB Aggregation_chunk0.txt\n",
      "Stored embedding for: MongoDB Aggregation_chunk1.txt\n",
      "Stored embedding for: MongoDB Aggregation_chunk2.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk0.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk1.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk10.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk11.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk2.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk3.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk4.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk5.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk6.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk7.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk8.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk9.txt\n",
      "Stored embedding for: MongoDB Examples_chunk0.txt\n",
      "Stored embedding for: MongoDB Examples_chunk1.txt\n",
      "Stored embedding for: MongoDB Examples_chunk2.txt\n",
      "Stored embedding for: MongoDB Examples_chunk3.txt\n",
      "Stored embedding for: MongoDB Examples_chunk4.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk0.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk1.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk2.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk3.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk4.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk5.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk6.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk7.txt\n",
      "Stored embedding for: Neo4j_chunk0.txt\n",
      "Stored embedding for: Neo4j_chunk1.txt\n",
      "Stored embedding for: Neo4j_chunk2.txt\n",
      "Stored embedding for: Neo4j_chunk3.txt\n",
      "Stored embedding for: Neo4j_chunk4.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk0.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk1.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk2.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk3.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk4.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk5.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk6.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk7.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk8.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk0.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk1.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk10.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk11.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk2.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk3.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk4.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk5.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk6.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk7.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk8.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk9.txt\n",
      "Stored embedding for: PyMongo_chunk0.txt\n",
      "Stored embedding for: PyMongo_chunk1.txt\n",
      "Stored embedding for: Redis + Python_chunk0.txt\n",
      "Stored embedding for: Redis + Python_chunk1.txt\n",
      "Stored embedding for: Redis + Python_chunk2.txt\n",
      "\n",
      "Total embedding time: 49.87 seconds\n",
      "Memory used for embeddings: 0.61 MB\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What question do you want to ask?  what is an AVL tree?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 2 matching chunks retrieved:\n",
      "\n",
      "Chunk 1: 12.6. B-Trees 12.6.1. B-Trees This module presents the B-tree. B-trees are usually attributed to R. Bayer and E. McCreight who described the B-tree in a 1972 paper. By 1979, B-trees had replaced virtually all large-file access methods other than hashing. B-trees, or some variant of B-trees, are the ...\n",
      "\n",
      "Chunk 2: Tree Visualization: Insert into a tree of degree 4 B+ 3/17/25, 9:22 AM 12.6. B-Trees — CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 5/9 Here is an exercise to see if you get the basic idea of tree insertion. To delete record from the tree, firs...\n",
      "\n",
      "Response from mistral:\n",
      " An AVL tree is a self-balancing binary search tree, named after its inventors Adel'son-Vel'skii and Landis. It maintains the balance of the tree by adjusting the height of the tree during insertions and deletions to ensure that the difference in heights between subtrees at any given node is no more than one. This helps to minimize the search time, as the tree remains relatively shallow even when it grows large.\n",
      "\n",
      "In contrast to B-trees, AVL trees are typically used for smaller datasets where efficiency is crucial and maintaining a balance between nodes is important. Both AVL trees and B-trees share similarities in that they aim to minimize the number of disk I/O operations by keeping related data close together, but AVL trees are primarily used for in-memory applications while B-trees are more commonly used for large datasets on disk.\n",
      "\n",
      "LLM query execution time: 35.69 seconds\n",
      "Memory used for LLM query: -467.15 MB\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import ollama\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Initialize ChromaDB connection\n",
    "chroma_client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "\n",
    "# Set constants\n",
    "COLLECTION_NAME = \"ds4300-rag\"\n",
    "TEXT_FOLDER = \"processed_texts\"  \n",
    "selected_model = None\n",
    "jina_model = None\n",
    "selected_llm_model = None\n",
    "\n",
    "# Function to get the current memory usage of the system in MB\n",
    "def get_system_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)  # Convert bytes to MB\n",
    "\n",
    "# Ensure collection exists and clear it at the start of each run\n",
    "def get_or_create_collection():\n",
    "    try:\n",
    "        chroma_client.delete_collection(COLLECTION_NAME)  # Clear existing data\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearing collection: {e}\")\n",
    "    return chroma_client.create_collection(COLLECTION_NAME)\n",
    "\n",
    "collection = get_or_create_collection()\n",
    "\n",
    "def get_embedding(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Generate an embedding for the given text using the selected embedding model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "            return jina_model.encode([text])[0].tolist()\n",
    "        else:\n",
    "            response = ollama.embeddings(model=EMBEDDING_MODEL, prompt=text)\n",
    "            return response[\"embedding\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for text: {e}\")\n",
    "        return None  # Return None if there's an error, and skip storing the embedding\n",
    "\n",
    "def store_embedding(doc_id: str, text: str, embedding: list):\n",
    "    \"\"\"\n",
    "    Store the document and its embedding in ChromaDB.\n",
    "    \"\"\"\n",
    "    if embedding is not None:  # Only store if embedding is valid\n",
    "        collection.add(ids=[doc_id], embeddings=[embedding], documents=[text])\n",
    "        print(f\"Stored embedding for: {doc_id}\")\n",
    "    else:\n",
    "        print(f\"Skipping file {doc_id} due to embedding generation error.\")\n",
    "\n",
    "def process_text_files():\n",
    "    \"\"\"\n",
    "    Reads text files, generates embeddings, and stores them in ChromaDB.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(TEXT_FOLDER):\n",
    "        print(f\"Folder '{TEXT_FOLDER}' not found.\")\n",
    "        return\n",
    "\n",
    "    text_files = [f for f in os.listdir(TEXT_FOLDER) if f.endswith(\".txt\")]\n",
    "    if not text_files:\n",
    "        print(\"No text files found.\")\n",
    "        return\n",
    "\n",
    "    # Record start time for embedding process\n",
    "    start_embedding_time = time.time()\n",
    "\n",
    "    # Get initial system memory usage\n",
    "    initial_system_memory = get_system_memory_usage()\n",
    "\n",
    "    for filename in text_files:\n",
    "        filepath = os.path.join(TEXT_FOLDER, filename)\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "                embedding = get_embedding(text)\n",
    "                store_embedding(filename, text, embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            continue  # Skip this file and continue with the next one\n",
    "\n",
    "    # Record end time for embedding process\n",
    "    end_embedding_time = time.time()\n",
    "\n",
    "    # Calculate total embedding time\n",
    "    embedding_time = end_embedding_time - start_embedding_time\n",
    "    print(f\"\\nTotal embedding time: {embedding_time:.2f} seconds\")\n",
    "    \n",
    "    # Get final system memory usage\n",
    "    final_system_memory = get_system_memory_usage()\n",
    "\n",
    "    # Calculate memory usage in MB\n",
    "    memory_used_for_embeddings = final_system_memory - initial_system_memory\n",
    "    print(f\"Memory used for embeddings: {memory_used_for_embeddings:.2f} MB\")\n",
    "\n",
    "def query_llm(query: str, matching_chunks: list) -> str:\n",
    "    \"\"\"\n",
    "    Query the LLM with a given question and relevant context.\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join([f\"Chunk {i+1}: {chunk}\" for i, chunk in enumerate(matching_chunks)])\n",
    "    prompt_to_send = (\n",
    "        f\"User's Question: {query}\\n\\n\"\n",
    "        f\"Relevant Context:\\n{context}\\n\\n\"\n",
    "        \"Your task: Answer the user's question as clearly as possible.\"\n",
    "    )\n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant with expertise in computer science.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_to_send}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "def perform_knn_search(query_text: str, k: int = 2):\n",
    "    \"\"\"\n",
    "    Perform a KNN similarity search in ChromaDB.\n",
    "    \"\"\"\n",
    "    # Get initial system memory before performing the LLM query\n",
    "    initial_system_memory = get_system_memory_usage()\n",
    "\n",
    "    # Start timer for KNN search and LLM query execution\n",
    "    start_query_time = time.time()\n",
    "\n",
    "    embedding = get_embedding(query_text)\n",
    "    if embedding is None:\n",
    "        print(\"Embedding generation failed, cannot perform search.\")\n",
    "        return\n",
    "\n",
    "    results = collection.query(query_embeddings=[embedding], n_results=k)\n",
    "\n",
    "    if not results['documents'][0]:\n",
    "        print(\"No relevant matches found.\")\n",
    "        return\n",
    "\n",
    "    matching_chunks = results['documents'][0]\n",
    "    print(f\"\\nTop {len(matching_chunks)} matching chunks retrieved:\")\n",
    "    for i, chunk in enumerate(matching_chunks):\n",
    "        print(f\"\\nChunk {i+1}: {chunk[:300]}...\")\n",
    "\n",
    "    response = query_llm(query_text, matching_chunks)\n",
    "    print(f\"\\nResponse from {LLM_MODEL}:\\n{response}\\n\")\n",
    "\n",
    "    # Record end time for the LLM query execution\n",
    "    end_query_time = time.time()\n",
    "\n",
    "    # Calculate time taken for KNN search and LLM query\n",
    "    query_time = end_query_time - start_query_time\n",
    "    print(f\"LLM query execution time: {query_time:.2f} seconds\")\n",
    "\n",
    "    # Get final system memory after performing the LLM query\n",
    "    final_system_memory = get_system_memory_usage()\n",
    "\n",
    "    # Calculate memory usage for the LLM query in MB\n",
    "    memory_used_for_query = final_system_memory - initial_system_memory\n",
    "    print(f\"Memory used for LLM query: {memory_used_for_query:.2f} MB\")\n",
    "\n",
    "\n",
    "# Prompt user to select an embedding model\n",
    "embedding_models = {\n",
    "    \"1\": \"nomic-embed-text\",\n",
    "    \"2\": \"jina-embeddings-v2-base-en\",\n",
    "    \"3\": \"granite-embedding:278m\",\n",
    "}\n",
    "\n",
    "print(\"Select an embedding model:\")\n",
    "for key, model in embedding_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_model not in embedding_models:\n",
    "    selected_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "EMBEDDING_MODEL = embedding_models[selected_model]\n",
    "\n",
    "# If Jina embeddings are selected, load the model\n",
    "if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "    jina_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True)\n",
    "\n",
    "# Prompt user to select an LLM model\n",
    "llm_models = {\n",
    "    \"1\": \"llama3.2:latest\",\n",
    "    \"2\": \"mistral\",\n",
    "}\n",
    "\n",
    "print(\"Select an LLM model:\")\n",
    "for key, model in llm_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_llm_model not in llm_models:\n",
    "    selected_llm_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "LLM_MODEL = llm_models[selected_llm_model]\n",
    "print(f\"Using LLM model: {LLM_MODEL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # process text files loads the parsed notes into the database\n",
    "    process_text_files()\n",
    "    query = input(\"What question do you want to ask? \")\n",
    "    # actually performs the semantic search and queries the LLM\n",
    "    perform_knn_search(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b01406d-1df6-4f1b-aa73-9121bfa795b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an embedding model:\n",
      "1: nomic-embed-text\n",
      "2: jina-embeddings-v2-base-en\n",
      "3: granite-embedding:278m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to your choice:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an LLM model:\n",
      "1: llama3.2:latest\n",
      "2: mistral\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to your choice:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM model: mistral\n",
      "Stored embedding for: AWS Intro_chunk0.txt\n",
      "Stored embedding for: AWS Intro_chunk1.txt\n",
      "Stored embedding for: AWS Intro_chunk2.txt\n",
      "Stored embedding for: AWS Intro_chunk3.txt\n",
      "Stored embedding for: AWS Intro_chunk4.txt\n",
      "Stored embedding for: AWS Intro_chunk5.txt\n",
      "Stored embedding for: B+Tree Walkthrough_chunk0.txt\n",
      "Stored embedding for: B+Tree Walkthrough_chunk1.txt\n",
      "Stored embedding for: B+Tree Walkthrough_chunk2.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk0.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk1.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk10.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk11.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk12.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk13.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk14.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk15.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk16.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk17.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk18.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk19.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk2.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk20.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk21.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk22.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk23.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk3.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk4.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk5.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk6.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk7.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk8.txt\n",
      "Stored embedding for: B-Trees — CS3 Data Structures & Algorithms_chunk9.txt\n",
      "Stored embedding for: B-trees_chunk0.txt\n",
      "Stored embedding for: B-trees_chunk1.txt\n",
      "Stored embedding for: B-trees_chunk2.txt\n",
      "Stored embedding for: B-trees_chunk3.txt\n",
      "Stored embedding for: B-trees_chunk4.txt\n",
      "Stored embedding for: B-trees_chunk5.txt\n",
      "Stored embedding for: BST_chunk0.txt\n",
      "Stored embedding for: BST_chunk1.txt\n",
      "Stored embedding for: BST_chunk10.txt\n",
      "Stored embedding for: BST_chunk11.txt\n",
      "Stored embedding for: BST_chunk12.txt\n",
      "Stored embedding for: BST_chunk2.txt\n",
      "Stored embedding for: BST_chunk3.txt\n",
      "Stored embedding for: BST_chunk4.txt\n",
      "Stored embedding for: BST_chunk5.txt\n",
      "Stored embedding for: BST_chunk6.txt\n",
      "Stored embedding for: BST_chunk7.txt\n",
      "Stored embedding for: BST_chunk8.txt\n",
      "Stored embedding for: BST_chunk9.txt\n",
      "Stored embedding for: BST_hw_chunk0.txt\n",
      "Stored embedding for: BST_hw_chunk1.txt\n",
      "Stored embedding for: BST_hw_chunk2.txt\n",
      "Stored embedding for: BST_hw_chunk3.txt\n",
      "Stored embedding for: Data Replication_chunk0.txt\n",
      "Stored embedding for: Data Replication_chunk1.txt\n",
      "Stored embedding for: Data Replication_chunk2.txt\n",
      "Stored embedding for: Data Replication_chunk3.txt\n",
      "Stored embedding for: Data Replication_chunk4.txt\n",
      "Stored embedding for: Data Replication_chunk5.txt\n",
      "Stored embedding for: Data Replication_chunk6.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk0.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk1.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk2.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk3.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk4.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk5.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk6.txt\n",
      "Stored embedding for: Document DBs and Mongo_chunk7.txt\n",
      "Stored embedding for: EC2 & Lambda_chunk0.txt\n",
      "Stored embedding for: EC2 & Lambda_chunk1.txt\n",
      "Stored embedding for: EC2 & Lambda_chunk2.txt\n",
      "Stored embedding for: EC2 & Lambda_chunk3.txt\n",
      "Stored embedding for: Foundations_chunk0.txt\n",
      "Stored embedding for: Foundations_chunk1.txt\n",
      "Stored embedding for: Foundations_chunk2.txt\n",
      "Stored embedding for: Foundations_chunk3.txt\n",
      "Stored embedding for: Foundations_chunk4.txt\n",
      "Stored embedding for: Foundations_chunk5.txt\n",
      "Stored embedding for: Introduction to Graph Data Model_chunk0.txt\n",
      "Stored embedding for: Introduction to Graph Data Model_chunk1.txt\n",
      "Stored embedding for: Introduction to Graph Data Model_chunk2.txt\n",
      "Stored embedding for: Introduction to Graph Data Model_chunk3.txt\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:65203/embedding\": read tcp 127.0.0.1:65205->127.0.0.1:65203: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping file MongoDB Aggregation_chunk0.txt due to embedding generation error.\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:65218/embedding\": read tcp 127.0.0.1:65220->127.0.0.1:65218: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping file MongoDB Aggregation_chunk1.txt due to embedding generation error.\n",
      "Stored embedding for: MongoDB Aggregation_chunk2.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk0.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk1.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk10.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk11.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk2.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk3.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk4.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk5.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk6.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk7.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk8.txt\n",
      "Stored embedding for: MongoDB Documentation_chunk9.txt\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:65234/embedding\": read tcp 127.0.0.1:65236->127.0.0.1:65234: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping file MongoDB Examples_chunk0.txt due to embedding generation error.\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:65247/embedding\": read tcp 127.0.0.1:65249->127.0.0.1:65247: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping file MongoDB Examples_chunk1.txt due to embedding generation error.\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:65255/embedding\": read tcp 127.0.0.1:65257->127.0.0.1:65255: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping file MongoDB Examples_chunk2.txt due to embedding generation error.\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:65261/embedding\": read tcp 127.0.0.1:65263->127.0.0.1:65261: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping file MongoDB Examples_chunk3.txt due to embedding generation error.\n",
      "Stored embedding for: MongoDB Examples_chunk4.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk0.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk1.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk2.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk3.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk4.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk5.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk6.txt\n",
      "Stored embedding for: Moving Beyond the Relational Model_chunk7.txt\n",
      "Stored embedding for: Neo4j_chunk0.txt\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:65267/embedding\": read tcp 127.0.0.1:65269->127.0.0.1:65267: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping file Neo4j_chunk1.txt due to embedding generation error.\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:65276/embedding\": read tcp 127.0.0.1:65278->127.0.0.1:65276: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping file Neo4j_chunk2.txt due to embedding generation error.\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:65282/embedding\": read tcp 127.0.0.1:65284->127.0.0.1:65282: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping file Neo4j_chunk3.txt due to embedding generation error.\n",
      "Stored embedding for: Neo4j_chunk4.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk0.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk1.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk2.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk3.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk4.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk5.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk6.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk7.txt\n",
      "Stored embedding for: NoSQL Documentation_chunk8.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk0.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk1.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk10.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk11.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk2.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk3.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk4.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk5.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk6.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk7.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk8.txt\n",
      "Stored embedding for: NoSQL Intro + KV DBs_chunk9.txt\n",
      "Stored embedding for: PyMongo_chunk0.txt\n",
      "Stored embedding for: PyMongo_chunk1.txt\n",
      "Stored embedding for: Redis + Python_chunk0.txt\n",
      "Error generating embedding for text: do embedding request: Post \"http://127.0.0.1:65288/embedding\": read tcp 127.0.0.1:65290->127.0.0.1:65288: wsarecv: An existing connection was forcibly closed by the remote host. (status code: 500)\n",
      "Skipping file Redis + Python_chunk1.txt due to embedding generation error.\n",
      "Stored embedding for: Redis + Python_chunk2.txt\n",
      "\n",
      "Total embedding time: 112.32 seconds\n",
      "Memory used for embeddings: 1.21 MB\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What question do you want to ask?  What is an AVL tree?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 2 matching chunks retrieved:\n",
      "\n",
      "Chunk 1: tree works. This would be the “ tree”, or a tree of order 3. Figure 12.6.2: An example of building a tree Next, let’s see how to search. Figure 12.6.3: An example of searching a tree Finally, let’s see an example of deleting from the tree B+ B+ 2 −3+ B+ 1 / 28 << < > >> Example 2-3+ Tree Visualizati...\n",
      "\n",
      "Chunk 2: on either side of the subtree pointer. (This generalizes the BST invariant.) 5. The root has at least two children if it is not a leaf. For example, the following is an order-5 B-tree (m=5) where the leaves have enough space to store up to 3 data records: Because the height of the tree is uniformly ...\n",
      "\n",
      "Response from mistral:\n",
      " An AVL tree is a self-balancing binary search tree, which means it maintains its height and balance during insertions, deletions, and lookups. The name \"AVL\" stands for Adelson-Velsky and Landis, who described the data structure in 1962.\n",
      "\n",
      "The main feature of an AVL tree is that it uses a balance factor for each node to keep track of the imbalance at that node. The balance factor for a node with two children (left subtree and right subtree) is calculated as the height difference between the left and right subtrees. This allows the tree to maintain a height roughly logarithmic in the number of nodes.\n",
      "\n",
      "The operations performed on an AVL tree, such as insertion, deletion, and searching, adjust the tree balance by performing rotations, which are operations that rearrange the nodes to restore balance. These rotations ensure that the height of the tree remains roughly logarithmic, providing efficient performance for these operations.\n",
      "\n",
      "In summary, an AVL tree is a type of balanced binary search tree, known for its ability to maintain balance during insertions, deletions, and lookups. It achieves this through the use of balance factors and rotations to adjust the structure when needed.\n",
      "\n",
      "LLM query execution time: 38.45 seconds\n",
      "Memory used for LLM query: -0.01 MB\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import ollama\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Initialize ChromaDB connection\n",
    "chroma_client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "\n",
    "# Set constants\n",
    "COLLECTION_NAME = \"ds4300-rag\"\n",
    "TEXT_FOLDER = \"processed_texts\"  \n",
    "selected_model = None\n",
    "jina_model = None\n",
    "selected_llm_model = None\n",
    "\n",
    "# Function to get the current memory usage of the system in MB\n",
    "def get_system_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)  # Convert bytes to MB\n",
    "\n",
    "def get_stable_memory_usage():\n",
    "    \"\"\"\n",
    "    Ensures that memory readings are stable by taking multiple samples\n",
    "    and ensuring no large fluctuations occur before accepting the reading.\n",
    "    \"\"\"\n",
    "    initial_memory = get_system_memory_usage()\n",
    "    time.sleep(0.1)  # Delay for a short time to allow system processes to stabilize\n",
    "    stable_memory = get_system_memory_usage()\n",
    "    \n",
    "    # If the memory fluctuates within 5% of the first reading, we take it as stable\n",
    "    while abs(stable_memory - initial_memory) > 0.05 * initial_memory:\n",
    "        time.sleep(0.1)  # Wait a bit before trying again\n",
    "        stable_memory = get_system_memory_usage()\n",
    "\n",
    "    return stable_memory\n",
    "\n",
    "# Ensure collection exists and clear it at the start of each run\n",
    "def get_or_create_collection():\n",
    "    try:\n",
    "        chroma_client.delete_collection(COLLECTION_NAME)  # Clear existing data\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearing collection: {e}\")\n",
    "    return chroma_client.create_collection(COLLECTION_NAME)\n",
    "\n",
    "collection = get_or_create_collection()\n",
    "\n",
    "def get_embedding(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Generate an embedding for the given text using the selected embedding model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "            return jina_model.encode([text])[0].tolist()\n",
    "        else:\n",
    "            response = ollama.embeddings(model=EMBEDDING_MODEL, prompt=text)\n",
    "            return response[\"embedding\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for text: {e}\")\n",
    "        return None  # Return None if there's an error, and skip storing the embedding\n",
    "\n",
    "def store_embedding(doc_id: str, text: str, embedding: list):\n",
    "    \"\"\"\n",
    "    Store the document and its embedding in ChromaDB.\n",
    "    \"\"\"\n",
    "    if embedding is not None:  # Only store if embedding is valid\n",
    "        collection.add(ids=[doc_id], embeddings=[embedding], documents=[text])\n",
    "        print(f\"Stored embedding for: {doc_id}\")\n",
    "    else:\n",
    "        print(f\"Skipping file {doc_id} due to embedding generation error.\")\n",
    "\n",
    "def process_text_files():\n",
    "    \"\"\"\n",
    "    Reads text files, generates embeddings, and stores them in ChromaDB.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(TEXT_FOLDER):\n",
    "        print(f\"Folder '{TEXT_FOLDER}' not found.\")\n",
    "        return\n",
    "\n",
    "    text_files = [f for f in os.listdir(TEXT_FOLDER) if f.endswith(\".txt\")]\n",
    "    if not text_files:\n",
    "        print(\"No text files found.\")\n",
    "        return\n",
    "\n",
    "    # Record start time for embedding process\n",
    "    start_embedding_time = time.time()\n",
    "\n",
    "    # Get initial system memory usage\n",
    "    initial_system_memory = get_stable_memory_usage()\n",
    "\n",
    "    for filename in text_files:\n",
    "        filepath = os.path.join(TEXT_FOLDER, filename)\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "                embedding = get_embedding(text)\n",
    "                store_embedding(filename, text, embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "            continue  # Skip this file and continue with the next one\n",
    "\n",
    "    # Record end time for embedding process\n",
    "    end_embedding_time = time.time()\n",
    "\n",
    "    # Calculate total embedding time\n",
    "    embedding_time = end_embedding_time - start_embedding_time\n",
    "    print(f\"\\nTotal embedding time: {embedding_time:.2f} seconds\")\n",
    "    \n",
    "    # Get final system memory usage\n",
    "    final_system_memory = get_stable_memory_usage()\n",
    "\n",
    "    # Calculate memory usage in MB\n",
    "    memory_used_for_embeddings = final_system_memory - initial_system_memory\n",
    "    print(f\"Memory used for embeddings: {memory_used_for_embeddings:.2f} MB\")\n",
    "\n",
    "def query_llm(query: str, matching_chunks: list) -> str:\n",
    "    \"\"\"\n",
    "    Query the LLM with a given question and relevant context.\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join([f\"Chunk {i+1}: {chunk}\" for i, chunk in enumerate(matching_chunks)])\n",
    "    prompt_to_send = (\n",
    "        f\"User's Question: {query}\\n\\n\"\n",
    "        f\"Relevant Context:\\n{context}\\n\\n\"\n",
    "        \"Your task: Answer the user's question as clearly as possible.\"\n",
    "    )\n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant with expertise in computer science.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_to_send}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "def perform_knn_search(query_text: str, k: int = 2):\n",
    "    \"\"\"\n",
    "    Perform a KNN similarity search in ChromaDB.\n",
    "    \"\"\"\n",
    "    # Get initial system memory before performing the LLM query\n",
    "    initial_system_memory = get_stable_memory_usage()\n",
    "\n",
    "    # Start timer for KNN search and LLM query execution\n",
    "    start_query_time = time.time()\n",
    "\n",
    "    embedding = get_embedding(query_text)\n",
    "    if embedding is None:\n",
    "        print(\"Embedding generation failed, cannot perform search.\")\n",
    "        return\n",
    "\n",
    "    results = collection.query(query_embeddings=[embedding], n_results=k)\n",
    "\n",
    "    if not results['documents'][0]:\n",
    "        print(\"No relevant matches found.\")\n",
    "        return\n",
    "\n",
    "    matching_chunks = results['documents'][0]\n",
    "    print(f\"\\nTop {len(matching_chunks)} matching chunks retrieved:\")\n",
    "    for i, chunk in enumerate(matching_chunks):\n",
    "        print(f\"\\nChunk {i+1}: {chunk[:300]}...\")\n",
    "\n",
    "    response = query_llm(query_text, matching_chunks)\n",
    "    print(f\"\\nResponse from {LLM_MODEL}:\\n{response}\\n\")\n",
    "\n",
    "    # Record end time for the LLM query execution\n",
    "    end_query_time = time.time()\n",
    "\n",
    "    # Calculate time taken for KNN search and LLM query\n",
    "    query_time = end_query_time - start_query_time\n",
    "    print(f\"LLM query execution time: {query_time:.2f} seconds\")\n",
    "\n",
    "    # Get final system memory after performing the LLM query\n",
    "    final_system_memory = get_stable_memory_usage()\n",
    "\n",
    "    # Calculate memory usage for the LLM query in MB\n",
    "    memory_used_for_query = final_system_memory - initial_system_memory\n",
    "    print(f\"Memory used for LLM query: {memory_used_for_query:.2f} MB\")\n",
    "\n",
    "\n",
    "# Prompt user to select an embedding model\n",
    "embedding_models = {\n",
    "    \"1\": \"nomic-embed-text\",\n",
    "    \"2\": \"jina-embeddings-v2-base-en\",\n",
    "    \"3\": \"granite-embedding:278m\",\n",
    "}\n",
    "\n",
    "print(\"Select an embedding model:\")\n",
    "for key, model in embedding_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_model not in embedding_models:\n",
    "    selected_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "EMBEDDING_MODEL = embedding_models[selected_model]\n",
    "\n",
    "# If Jina embeddings are selected, load the model\n",
    "if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "    jina_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True)\n",
    "\n",
    "# Prompt user to select an LLM model\n",
    "llm_models = {\n",
    "    \"1\": \"llama3.2:latest\",\n",
    "    \"2\": \"mistral\",\n",
    "}\n",
    "\n",
    "print(\"Select an LLM model:\")\n",
    "for key, model in llm_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_llm_model not in llm_models:\n",
    "    selected_llm_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "LLM_MODEL = llm_models[selected_llm_model]\n",
    "print(f\"Using LLM model: {LLM_MODEL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # process text files loads the parsed notes into the database\n",
    "    process_text_files()\n",
    "    query = input(\"What question do you want to ask? \")\n",
    "    # actually performs the semantic search and queries the LLM\n",
    "    perform_knn_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2befd-91d0-4b35-8671-04b8563b163b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
