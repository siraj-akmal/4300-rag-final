{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43a70574-bd1f-425e-8058-663c202d80f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an embedding model:\n",
      "1: nomic-embed-text\n",
      "2: jina-embeddings-v2-base-en\n",
      "3: granite-embedding:278m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to your choice:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an LLM model:\n",
      "1: llama3.2:latest\n",
      "2: mistral\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to your choice:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM model: mistral\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What question do you want to ask?  what is an AVL tree?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation took 3.0749 seconds.\n",
      "Memory used: 0.20 MB.\n",
      "\n",
      "Top 2 matching chunks retrieved:\n",
      "\n",
      "Chunk 1: So, we will view the process as follows: A key x from the keys is selected uniformly at random and is inserted to the tree. Then all the other keys are inserted. Here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree. Thus, the height...\n",
      "\n",
      "Chunk 2: Analysis We claim that for all n ≥1 E[Yn] ≤1 4 \u0010n+3 3 \u0011 . We prove this by induction on n. Base case: E[Y1] = 20 = 1. Induction step: We have E[Yn] ≤4 n n−1 X i=1 E[Yi] Using the fact that n−1 X i=0 \u0010i + 3 3 \u0011 = \u0010n + 3 4 \u0011 E[Yn] ≤4 n · 1 4 · \u0010n + 3 4 \u0011 E[Yn] ≤1 4 · \u0010n + 3 3 \u0011 25 Jensen’s inequality ...\n",
      "Query processing took 32.9691 seconds.\n",
      "Memory used: 0.04 MB.\n",
      "\n",
      "Response from mistral:\n",
      " An AVL tree is a self-balancing binary search tree, which was invented by Georgii Adelson-Velsky and Evgenii Landis in 1962. The main feature of an AVL tree is that it maintains the height difference between the two subtrees at each node within a range of -1 and +1, ensuring that the tree remains approximately balanced during insertions and deletions. This property ensures that the worst-case time complexity for search, insertion, and removal operations is O(log n). The name AVL tree comes from the initials of the inventors' last names in Russian (A.S. Adelson-Velsky and E.M. Landis).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TESTING REDIS\n",
    "import redis\n",
    "import numpy as np\n",
    "import os\n",
    "from redis.commands.search.query import Query\n",
    "from transformers import AutoModel\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Initialize Redis connection\n",
    "redis_client = redis.Redis(host=\"localhost\", port=6379, db=0)\n",
    "\n",
    "# set constants\n",
    "VECTOR_DIM = 768\n",
    "INDEX_NAME = \"embedding_index\"\n",
    "DOC_PREFIX = \"doc:\"\n",
    "DISTANCE_METRIC = \"COSINE\"\n",
    "TEXT_FOLDER = \"processed_texts\"  \n",
    "selected_model = None\n",
    "jina_model = None\n",
    "selected_llm_model = None\n",
    "\n",
    "#clear redis database if reindexing\n",
    "def create_hnsw_index():\n",
    "    try:\n",
    "        redis_client.execute_command(f\"FT.DROPINDEX {INDEX_NAME} DD\")\n",
    "    except redis.exceptions.ResponseError:\n",
    "        pass\n",
    "    \n",
    "    redis_client.execute_command(\n",
    "        f\"\"\"\n",
    "        FT.CREATE {INDEX_NAME} ON HASH PREFIX 1 {DOC_PREFIX}\n",
    "        SCHEMA text TEXT\n",
    "        embedding VECTOR HNSW 6 DIM {VECTOR_DIM} TYPE FLOAT32 DISTANCE_METRIC {DISTANCE_METRIC}\n",
    "        \"\"\"\n",
    "    )\n",
    "    print(\"Index created successfully.\")\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"\n",
    "    Returns memory usage of the current process in MB.\n",
    "    \"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / (1024 * 1024)  # in MB\n",
    "\n",
    "def get_embedding(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Generate an embedding for the given text using the selected embedding model.\n",
    "\n",
    "    This function uses either the Jina embeddings model or the Ollama embeddings\n",
    "    model based on the global EMBEDDING_MODEL setting.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of floats representing the embedding vector for the input text.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start timer\n",
    "    initial_memory = get_memory_usage()  # Record memory usage before operation\n",
    "    \n",
    "    if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "        embedding = jina_model.encode([text])[0].tolist()\n",
    "    else:\n",
    "        response = ollama.embeddings(model=EMBEDDING_MODEL, prompt=text)\n",
    "        embedding = response[\"embedding\"]\n",
    "    \n",
    "    end_time = time.time()  # End timer\n",
    "    final_memory = get_memory_usage()  # Record memory usage after operation\n",
    "\n",
    "    print(f\"Embedding generation took {end_time - start_time:.4f} seconds.\")\n",
    "    print(f\"Memory used: {final_memory - initial_memory:.2f} MB.\")\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def store_embedding(doc_id: str, text: str, embedding: list):\n",
    "    key = f\"{DOC_PREFIX}{doc_id}\"\n",
    "    redis_client.hset(\n",
    "        key,\n",
    "        mapping={\n",
    "            \"text\": text,\n",
    "            \"embedding\": np.array(embedding, dtype=np.float32).tobytes(),  # Store as byte array\n",
    "        },\n",
    "    )\n",
    "    print(f\"Stored embedding for: {doc_id}\")\n",
    "\n",
    "def process_text_files():\n",
    "    \"\"\"\n",
    "    This function processes all text files in the specified folder, reads their content,\n",
    "    generates embeddings for the text using the selected embedding model, and stores the\n",
    "    embeddings along with the text content in Redis.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(TEXT_FOLDER):\n",
    "        print(f\"Folder '{TEXT_FOLDER}' not found.\")\n",
    "        return\n",
    "\n",
    "    text_files = [f for f in os.listdir(TEXT_FOLDER) if f.endswith(\".txt\")]\n",
    "    if not text_files:\n",
    "        print(\"No text files found.\")\n",
    "        return\n",
    "\n",
    "    for filename in text_files:\n",
    "        filepath = os.path.join(TEXT_FOLDER, filename)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            embedding = get_embedding(text)\n",
    "            store_embedding(filename, text, embedding)\n",
    "\n",
    "def query_llm(query: str, matching_chunks: list) -> str:\n",
    "    \"\"\"\n",
    "    Query the Language Model (LLM) with a given question and relevant context.\n",
    "\n",
    "    This function prepares a prompt by combining the user's query and relevant context\n",
    "    from matching chunks. It then sends this prompt to the LLM for processing and returns\n",
    "    the model's response.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The user's question or input to be answered by the LLM.\n",
    "    matching_chunks (list): A list of text chunks that provide relevant context for the query.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start timer\n",
    "    initial_memory = get_memory_usage()  # Record memory usage before operation\n",
    "    \n",
    "    context = \"\\n\\n\".join([f\"Chunk {i+1}: {chunk}\" for i, chunk in enumerate(matching_chunks)])\n",
    "    prompt_to_send = (\n",
    "        f\"User's Question: {query}\\n\\n\"\n",
    "        f\"Relevant Context (if applicable):\\n{context}\\n\\n\"\n",
    "        \"Your task: Answer the user's question as clearly and accurately as possible.\"\n",
    "        \"If the question is unclear or not actually a question, state that explicitly.\"\n",
    "    )\n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant with expertise in computer science.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_to_send}\n",
    "        ],\n",
    "    )\n",
    "    end_time = time.time()  # End timer\n",
    "    final_memory = get_memory_usage()  # Record memory usage after operation\n",
    "\n",
    "    print(f\"Query processing took {end_time - start_time:.4f} seconds.\")\n",
    "    print(f\"Memory used: {final_memory - initial_memory:.2f} MB.\")\n",
    "    \n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "def perform_knn_search(query_text: str, k: int = 2):\n",
    "    \"\"\"\n",
    "    Perform a K-Nearest Neighbors (KNN) search on the Redis index using the given query text.\n",
    "\n",
    "    This function embeds the query text, searches for similar embeddings in the Redis index,\n",
    "    retrieves matching text chunks, and generates a response using a language model.\n",
    "\n",
    "    Parameters:\n",
    "    query_text (str): The text query to search for in the index.\n",
    "    k (int, optional): The number of nearest neighbors to retrieve. Defaults to 2.\n",
    "    \"\"\"\n",
    "    embedding = get_embedding(query_text)\n",
    "    q = (\n",
    "        Query(f\"*=>[KNN {k} @embedding $vec AS vector_distance]\")\n",
    "        .sort_by(\"vector_distance\")\n",
    "        .return_fields(\"text\", \"vector_distance\")\n",
    "        .dialect(2)\n",
    "    )\n",
    "    res = redis_client.ft(INDEX_NAME).search(\n",
    "        q, query_params={\"vec\": np.array(embedding, dtype=np.float32).tobytes()}\n",
    "    )\n",
    "    matching_chunks = [doc.text for doc in res.docs]\n",
    "    if not matching_chunks:\n",
    "        print(\"No relevant matches found.\")\n",
    "        return\n",
    "    print(f\"\\nTop {len(matching_chunks)} matching chunks retrieved:\")\n",
    "    for i, chunk in enumerate(matching_chunks):\n",
    "        print(f\"\\nChunk {i+1}: {chunk[:300]}...\")  # Display first 300 characters\n",
    "    response = query_llm(query_text, matching_chunks)\n",
    "    print(f\"\\nResponse from {LLM_MODEL}:\\n{response}\\n\")\n",
    "\n",
    "# Prompt user to select an embedding model\n",
    "embedding_models = {\n",
    "    \"1\": \"nomic-embed-text\",\n",
    "    \"2\": \"jina-embeddings-v2-base-en\",\n",
    "    \"3\": \"granite-embedding:278m\",\n",
    "}\n",
    "\n",
    "print(\"Select an embedding model:\")\n",
    "for key, model in embedding_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_model not in embedding_models:\n",
    "    selected_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "EMBEDDING_MODEL = embedding_models[selected_model]\n",
    "\n",
    "# If Jina embeddings are selected, load the model\n",
    "if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "    jina_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True)\n",
    "\n",
    "# Prompt user to select an LLM model\n",
    "llm_models = {\n",
    "    \"1\": \"llama3.2:latest\",\n",
    "    \"2\": \"mistral\",\n",
    "}\n",
    "\n",
    "print(\"Select an LLM model:\")\n",
    "for key, model in llm_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_llm_model not in llm_models:\n",
    "    selected_llm_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "LLM_MODEL = llm_models[selected_llm_model]\n",
    "print(f\"Using LLM model: {LLM_MODEL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # process text files loads the parsed notes into the database\n",
    "    #process_text_files()\n",
    "    query = input(\"What question do you want to ask? \")\n",
    "    # acctually performs the semantic search and queries the LLM\n",
    "    perform_knn_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b9cef2-69eb-4a12-9d86-893a9bd58c53",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chromadb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#TESTING CHROMA\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mollama\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'"
     ]
    }
   ],
   "source": [
    "#TESTING CHROMA\n",
    "import chromadb\n",
    "import ollama\n",
    "import os\n",
    "from transformers import AutoModel\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Initialize ChromaDB connection\n",
    "chroma_client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "\n",
    "# Set constants\n",
    "COLLECTION_NAME = \"ds4300-rag\"\n",
    "TEXT_FOLDER = \"processed_texts\"  \n",
    "selected_model = None\n",
    "jina_model = None\n",
    "selected_llm_model = None\n",
    "\n",
    "# Ensure collection exists and clear it at the start of each run\n",
    "def get_or_create_collection():\n",
    "    try:\n",
    "        chroma_client.delete_collection(COLLECTION_NAME)  # Clear existing data\n",
    "    except:\n",
    "        pass  # Collection might not exist yet\n",
    "    return chroma_client.create_collection(COLLECTION_NAME)\n",
    "\n",
    "collection = get_or_create_collection()\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"\n",
    "    Returns memory usage of the current process in MB.\n",
    "    \"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / (1024 * 1024)  # in MB\n",
    "\n",
    "def get_embedding(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Generate an embedding for the given text using the selected embedding model.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start timer\n",
    "    initial_memory = get_memory_usage()  # Record memory usage before operation\n",
    "    \n",
    "    if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "        embedding = jina_model.encode([text])[0].tolist()\n",
    "    else:\n",
    "        response = ollama.embeddings(model=EMBEDDING_MODEL, prompt=text)\n",
    "        embedding = response[\"embedding\"]\n",
    "    \n",
    "    end_time = time.time()  # End timer\n",
    "    final_memory = get_memory_usage()  # Record memory usage after operation\n",
    "\n",
    "    print(f\"Embedding generation took {end_time - start_time:.4f} seconds.\")\n",
    "    print(f\"Memory used: {final_memory - initial_memory:.2f} MB.\")\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def store_embedding(doc_id: str, text: str, embedding: list):\n",
    "    \"\"\"\n",
    "    Store the document and its embedding in ChromaDB.\n",
    "    \"\"\"\n",
    "    collection.add(ids=[doc_id], embeddings=[embedding], documents=[text])\n",
    "    print(f\"Stored embedding for: {doc_id}\")\n",
    "\n",
    "def process_text_files():\n",
    "    \"\"\"\n",
    "    Reads text files, generates embeddings, and stores them in ChromaDB.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(TEXT_FOLDER):\n",
    "        print(f\"Folder '{TEXT_FOLDER}' not found.\")\n",
    "        return\n",
    "\n",
    "    text_files = [f for f in os.listdir(TEXT_FOLDER) if f.endswith(\".txt\")]\n",
    "    if not text_files:\n",
    "        print(\"No text files found.\")\n",
    "        return\n",
    "\n",
    "    for filename in text_files:\n",
    "        filepath = os.path.join(TEXT_FOLDER, filename)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            embedding = get_embedding(text)\n",
    "            store_embedding(filename, text, embedding)\n",
    "\n",
    "def query_llm(query: str, matching_chunks: list) -> str:\n",
    "    \"\"\"\n",
    "    Query the LLM with a given question and relevant context.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start timer\n",
    "    initial_memory = get_memory_usage()  # Record memory usage before operation\n",
    "    \n",
    "    context = \"\\n\\n\".join([f\"Chunk {i+1}: {chunk}\" for i, chunk in enumerate(matching_chunks)])\n",
    "    prompt_to_send = (\n",
    "        f\"User's Question: {query}\\n\\n\"\n",
    "        f\"Relevant Context:\\n{context}\\n\\n\"\n",
    "        \"Your task: Answer the user's question as clearly as possible.\"\n",
    "    )\n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant with expertise in computer science.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_to_send}]\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()  # End timer\n",
    "    final_memory = get_memory_usage()  # Record memory usage after operation\n",
    "\n",
    "    print(f\"Query processing took {end_time - start_time:.4f} seconds.\")\n",
    "    print(f\"Memory used: {final_memory - initial_memory:.2f} MB.\")\n",
    "    \n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "def perform_knn_search(query_text: str, k: int = 2):\n",
    "    \"\"\"\n",
    "    Perform a KNN similarity search in ChromaDB.\n",
    "    \"\"\"\n",
    "    embedding = get_embedding(query_text)\n",
    "    results = collection.query(query_embeddings=[embedding], n_results=k)\n",
    "\n",
    "    if not results['documents'][0]:\n",
    "        print(\"No relevant matches found.\")\n",
    "        return\n",
    "\n",
    "    matching_chunks = results['documents'][0]\n",
    "    print(f\"\\nTop {len(matching_chunks)} matching chunks retrieved:\")\n",
    "    for i, chunk in enumerate(matching_chunks):\n",
    "        print(f\"\\nChunk {i+1}: {chunk[:300]}...\")\n",
    "\n",
    "    response = query_llm(query_text, matching_chunks)\n",
    "    print(f\"\\nResponse from {LLM_MODEL}:\\n{response}\\n\")\n",
    "\n",
    "# Prompt user to select an embedding model\n",
    "embedding_models = {\n",
    "    \"1\": \"nomic-embed-text\",\n",
    "    \"2\": \"jina-embeddings-v2-base-en\",\n",
    "    \"3\": \"granite-embedding:278m\",\n",
    "}\n",
    "\n",
    "print(\"Select an embedding model:\")\n",
    "for key, model in embedding_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_model not in embedding_models:\n",
    "    selected_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "EMBEDDING_MODEL = embedding_models[selected_model]\n",
    "\n",
    "# If Jina embeddings are selected, load the model\n",
    "if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "    jina_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True)\n",
    "\n",
    "# Prompt user to select an LLM model\n",
    "llm_models = {\n",
    "    \"1\": \"llama3.2:latest\",\n",
    "    \"2\": \"mistral\",\n",
    "}\n",
    "\n",
    "print(\"Select an LLM model:\")\n",
    "for key, model in llm_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_llm_model not in llm_models:\n",
    "    selected_llm_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "LLM_MODEL = llm_models[selected_llm_model]\n",
    "print(f\"Using LLM model: {LLM_MODEL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # process text files loads the parsed notes into the database\n",
    "    #process_text_files()\n",
    "    query = input(\"What question do you want to ask? \")\n",
    "    # actually performs the semantic search and queries the LLM\n",
    "    perform_knn_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eda888b-d4f4-4cea-9cb5-89f7e69e66b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chroma-hnswlib\n",
      "  Using cached chroma_hnswlib-0.7.6.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\ryanw\\anaconda3\\lib\\site-packages (from chroma-hnswlib) (1.26.4)\n",
      "Building wheels for collected packages: chroma-hnswlib\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): started\n",
      "  Building wheel for chroma-hnswlib (pyproject.toml): finished with status 'error'\n",
      "Failed to build chroma-hnswlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for chroma-hnswlib (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [5 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'hnswlib' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for chroma-hnswlib\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (chroma-hnswlib)\n"
     ]
    }
   ],
   "source": [
    "!pip install chroma-hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba0a69-0921-41a2-9b27-5fcf4bff3651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
