{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e411128-30f8-45f0-b277-ceaa201ba898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an embedding model:\n",
      "1: nomic-embed-text\n",
      "2: jina-embeddings-v2-base-en\n",
      "3: granite-embedding:278m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an LLM model:\n",
      "1: llama3.2:latest\n",
      "2: mistral\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number corresponding to your choice:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM model: mistral\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What question do you want to ask? (Type 'exit' to quit)  \"What are the benefits of BSON over JSON in MongoDB?\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from mistral: 1. Embedded Data Structures: BSON (Binary JSON) allows for embedded data structures, which means that you can store complex data types like arrays and nested documents within a single document without needing to reference other documents. This can lead to more efficient data retrieval as opposed to having to make multiple requests.\n",
      "\n",
      "2. Timestamps: BSON includes a Date data type, which is not natively supported in JSON. This allows for easier handling of timestamps and date-based operations within your MongoDB documents.\n",
      "\n",
      "3. Maximum Data Size: BSON supports larger maximum document size compared to JSON. While a JSON object can have a maximum size of 2GB, a BSON document can be up to 16MB (MongoDB 3.2 or later) and 4MB (previous versions). This is crucial when dealing with large data sets.\n",
      "\n",
      "4. Regular Expressions: BSON includes native support for JavaScript regular expressions, which can be useful for various querying purposes.\n",
      "\n",
      "5. Minimized Data Transfer: While JSON is text-based, BSON is a binary format that minimizes the amount of data being transferred due to its more compact representation. This results in faster data transfer over the network.\n",
      "\n",
      "6. Int32 and Long Data Types: BSON supports 64-bit integers (Int32 and Long) as opposed to JSON's 53-bit signed integer limit. This can be beneficial when dealing with large integer values.\n",
      "\n",
      "7. Auto-Discovery of Fields: When inserting new documents, MongoDB automatically indexes the fields in BSON documents, whereas JSON requires manual indexing or additional tools like mongoose (if using Node.js) to perform this task.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What question do you want to ask? (Type 'exit' to quit)  What is the average LSAT score for St. Johns?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from mistral:  The average LSAT (Law School Admission Test) score for St. John's University School of Law is typically around 154 - 156, as of my last update. However, it's important to note that these numbers can vary from year to year and are subject to change. I recommend visiting the school's official website or contacting their admissions office for the most current data. Good luck with your law school applications!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What question do you want to ask? (Type 'exit' to quit)  how do i play hulk in marvel rivals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from mistral:  To play as Hulk in Marvel Rivals (which I believe is a typo and you meant Marvel: Ultimate Alliance 3), follow these steps:\n",
      "\n",
      "1. Purchase the game: Marvel: Ultimate Alliance 3 can be found on various platforms such as Nintendo Switch, PC, Xbox One, and PlayStation 4.\n",
      "\n",
      "2. Create or select a character: In the main menu, choose \"Start New Game\" to begin your journey, or load an existing save if you have one. After selecting a team name and your first three characters (you can change them later), you'll be taken to the map screen.\n",
      "\n",
      "3. Unlock Hulk: To unlock Hulk, you must complete the storyline up to a certain point. Progress through each level until you reach Chapter 6 or 7 in the campaign.\n",
      "\n",
      "4. Recruit Hulk: Once you've reached that part of the story, you'll receive an opportunity to recruit Hulk for your team by completing a special mission. This mission will typically involve defeating a group of enemies in a specific location, often with certain conditions like limiting the number of team members or using specific character abilities.\n",
      "\n",
      "5. Add Hulk to your roster: After successfully completing the mission, Hulk should become available for selection on your character roster. You can then switch out one of your existing characters with Hulk and use him in battles as you see fit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What question do you want to ask? (Type 'exit' to quit)  what is the TV show invincible about\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from mistral:  \"Invincible\" is an American adult animated superhero drama television series that premiered on Amazon Prime Video in March 2021. The series is based on the comic book series of the same name by Robert Kirkman, Cory Walker, and Ryan Ottley.\n",
      "\n",
      "The story follows Mark Grayson (voiced by Steven Yeun), a normal teenager who is just like any other boy â€“ except that his father, Nolan Grayson (J.K. Simmons), is the most powerful superhero on the planet, Omni-Man. As Mark develops powers of his own, he enters into his father's vast and perilous universe to learn how to control them. However, things are not as straightforward as they seem, and Mark soon finds himself in situations that force him to confront the complexities and cruelty that come with being a superhero. The series is known for its dark themes, character development, and exploration of moral complexity within the superhero genre.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Constants\n",
    "VECTOR_DIM = 768\n",
    "TEXT_FOLDER = \"processed_texts\"  \n",
    "selected_model = None\n",
    "selected_llm_model = None\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Generate an embedding for the given text using the selected embedding model.\n",
    "    \"\"\"\n",
    "    if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "        return jina_model.encode([text])[0].tolist()\n",
    "    else:\n",
    "        response = ollama.embeddings(model=EMBEDDING_MODEL, prompt=text)\n",
    "        return response[\"embedding\"]\n",
    "\n",
    "# Function to query the LLM\n",
    "def query_llm(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Query the Language Model (LLM) with the user's query.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# Prompt user to select an embedding model\n",
    "embedding_models = {\n",
    "    \"1\": \"nomic-embed-text\",\n",
    "    \"2\": \"jina-embeddings-v2-base-en\",\n",
    "    \"3\": \"granite-embedding:278m\",\n",
    "}\n",
    "\n",
    "print(\"Select an embedding model:\")\n",
    "for key, model in embedding_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_model not in embedding_models:\n",
    "    selected_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "EMBEDDING_MODEL = embedding_models[selected_model]\n",
    "\n",
    "# If Jina embeddings are selected, load the model\n",
    "if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "    jina_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True)\n",
    "\n",
    "# Prompt user to select an LLM model\n",
    "llm_models = {\n",
    "    \"1\": \"llama3.2:latest\",\n",
    "    \"2\": \"mistral\",\n",
    "}\n",
    "\n",
    "print(\"Select an LLM model:\")\n",
    "for key, model in llm_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_llm_model not in llm_models:\n",
    "    selected_llm_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "LLM_MODEL = llm_models[selected_llm_model]\n",
    "print(f\"Using LLM model: {LLM_MODEL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        # Prompt the user for a query\n",
    "        query = input(\"What question do you want to ask? (Type 'exit' to quit) \")\n",
    "        \n",
    "        # If the user types 'exit', break the loop and stop the program\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Exiting program.\")\n",
    "            break\n",
    "        \n",
    "        # Perform the query using the selected LLM model\n",
    "        response = query_llm(query)\n",
    "        print(f\"Response from {LLM_MODEL}: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577f504-910a-432e-b5c7-5a3dbc8ee9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Constants\n",
    "VECTOR_DIM = 768\n",
    "TEXT_FOLDER = \"processed_texts\"  \n",
    "selected_model = None\n",
    "selected_llm_model = None\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Generate an embedding for the given text using the selected embedding model.\n",
    "    \"\"\"\n",
    "    if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "        return jina_model.encode([text])[0].tolist()\n",
    "    else:\n",
    "        response = ollama.embeddings(model=EMBEDDING_MODEL, prompt=text)\n",
    "        return response[\"embedding\"]\n",
    "\n",
    "# Function to query the LLM\n",
    "def query_llm(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Query the Language Model (LLM) with the user's query.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# Prompt user to select an embedding model\n",
    "embedding_models = {\n",
    "    \"1\": \"nomic-embed-text\",\n",
    "    \"2\": \"jina-embeddings-v2-base-en\",\n",
    "    \"3\": \"granite-embedding:278m\",\n",
    "}\n",
    "\n",
    "print(\"Select an embedding model:\")\n",
    "for key, model in embedding_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_model not in embedding_models:\n",
    "    selected_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "EMBEDDING_MODEL = embedding_models[selected_model]\n",
    "\n",
    "# If Jina embeddings are selected, load the model\n",
    "if EMBEDDING_MODEL == \"jina-embeddings-v2-base-en\":\n",
    "    jina_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True)\n",
    "\n",
    "# Prompt user to select an LLM model\n",
    "llm_models = {\n",
    "    \"1\": \"llama3.2:latest\",\n",
    "    \"2\": \"mistral\",\n",
    "}\n",
    "\n",
    "print(\"Select an LLM model:\")\n",
    "for key, model in llm_models.items():\n",
    "    print(f\"{key}: {model}\")\n",
    "\n",
    "while selected_llm_model not in llm_models:\n",
    "    selected_llm_model = input(\"Enter the number corresponding to your choice: \")\n",
    "\n",
    "LLM_MODEL = llm_models[selected_llm_model]\n",
    "print(f\"Using LLM model: {LLM_MODEL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        # Prompt the user for a query\n",
    "        query = input(\"What question do you want to ask? (Type 'exit' to quit) \")\n",
    "        \n",
    "        # If the user types 'exit', break the loop and stop the program\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Exiting program.\")\n",
    "            break\n",
    "        \n",
    "        # Perform the query using the selected LLM model\n",
    "        response = query_llm(query)\n",
    "        print(f\"Response from {LLM_MODEL}: {response}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
